{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Paris Saclay Center for Data Science](http://www.datascience-paris-saclay.fr)\n",
    "\n",
    "## [RAMP](https://www.ramp.studio/problems/MNIST_simplified) MNIST handwritten digit classification\n",
    "\n",
    "_Mehdi Cherti (CNRS), Balázs Kégl (CNRS)_\n",
    "    \n",
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "\n",
    "## The prediction task\n",
    "\n",
    "The goal of this RAMP is to classify correctly handwritten digits. For each submission, you will have to provide an image classifier (versus the original setup that required a transformer and a batch classifier). The images are usually big so loading them into the memory at once may be impossible. The image classifier therefore will access them through an `img_loader` function which can load one image at a time. \n",
    "\n",
    "## Hints\n",
    "\n",
    "Setting up an AWS instance is easy, just follow [this tutorial](https://medium.com/@mateuszsieniawski/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac#.dariq7i2u).\n",
    "\n",
    "For learning the nuts and bolts of convolutional nets, we suggest that you follow [Andrej Karpathy’s excellent course](http://cs231n.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the images are not yet in `data/imgs`, change the type of the net cell to \"Code\" and run it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_df = df['id']\n",
    "y_df = df['class']\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id\n",
       "class      \n",
       "0      4742\n",
       "1      5370\n",
       "2      4795\n",
       "3      4914\n",
       "4      4675\n",
       "5      4337\n",
       "6      4709\n",
       "7      4998\n",
       "8      4707\n",
       "9      4753"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts_df = df.groupby('class').count()\n",
    "labels_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worthwhile to look at some image panels, grouped by label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kegl/anaconda/lib/python2.7/site-packages/matplotlib/figure.py:1999: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not compatible \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw/CCTTaGYXjBJhvDMLxgk41hGF74f3fnFxdCtYRJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c145e1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_rows = 4\n",
    "nb_cols = 4\n",
    "nb_elements = nb_rows * nb_cols\n",
    "label = 8\n",
    "\n",
    "df_given_label = df[df['class']==label]\n",
    "\n",
    "subsample = np.random.choice(df_given_label['id'], replace=False, size=nb_elements)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "grid = AxesGrid(fig, 111, # similar to subplot(141)\n",
    "                nrows_ncols = (nb_rows, nb_cols),\n",
    "                axes_pad = 0.05,\n",
    "                label_mode = \"1\",\n",
    ")\n",
    "for i, image_id in enumerate(subsample):\n",
    "    filename = 'data/imgs/{}'.format(image_id)\n",
    "    image = imread(filename)\n",
    "    im = grid[i].imshow(image, cmap='Greys', interpolation='nearest')\n",
    "    grid[i].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images have size 28 $\\times$ 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_subsample = 1000\n",
    "shapes = np.empty((n_subsample, 2))\n",
    "for i, image_id in enumerate(X_df[:n_subsample]):\n",
    "    filename = 'data/imgs/{}'.format(image_id)\n",
    "    image = imread(filename)\n",
    "    shapes[i] = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <th>28.0</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "height width       \n",
       "28.0   28.0    1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes_df = pd.DataFrame(shapes, columns=['height', 'width'])\n",
    "shapes_df['count'] = 0\n",
    "shapes_df.groupby(['height', 'width']).count().sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c165c6090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c165dc490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shapes_df['height'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing\n",
    "\n",
    "In the first workflow element image_preprocessor.py you can resize, crop, or rotate the images. This is an important step. Neural nets need standard-size images defined by the dimension of the input layer. MNIST images are centered and resized, so these operations are unlikely to be useful but rotation may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c165e6050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = 'data/imgs/{}'.format(X_df[161])\n",
    "image = imread(filename)\n",
    "plt.imshow(image, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we resize the images to different resolutions, then blow them up so the difference can be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kegl/anaconda/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAACXCAYAAADOH0Q1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAA4pJREFUeJzt3T9KK1EYh+GMBtTWRrCzdAdKEMli\nxEIE3YM7cAWW4jpcg7oAFXs70dzGCxb3TK6H+WXy53mqwJeJByOvB06YNJPJZACQsNb3AoDlJTBA\njMAAMQIDxAgMECMwQIzAADECA8QIDBAz7HsB33ycePk0Px57f5dPM/0pdjBAkMAAMQIDxAgMECMw\nQIzAADECA8QIDBAjMECMwAAxAgPECAwQIzBAjMAAMfNyu4aF8vj4WJy9vb0VZ8fHx4nl0LH9/f2q\n6x4eHjpeyeKzgwFiBAaIERggRmCAGIEBYgQGiHFMXWFnZ6c4G4/Hxdnr62tiOTC37GCAGIEBYgQG\niBEYIEZggBiBAWIcU1fY2toqzt7f34uzl5eX1tfd3d2tXhPdOTk56XsJS8MOBogRGCBGYIAYgQFi\nBAaIERggRmCAGJ+DqbC5uVmcjUaj4uz29rb1dS8vL6vXBPPIDgaIERggRmCAGIEBYgQGiBEYIMYx\ndcf29vaKs+fn5xmuhFpnZ2d9L2Fp2MEAMQIDxAgMECMwQIzAADECA8Q4pu7YcOhXCn/ZwQAxAgPE\nCAwQIzBAjMAAMQIDxDhT7djBwUFxdnd3N8OVUGtjY6Pquuvr6+qfeX5+Xn3tPLODAWIEBogRGCBG\nYIAYgQFiBAaIERggxudgOnZ4eFicnZ6etl779fVVnK2t+V/A4vFXC8QIDBAjMECMwAAxAgPECAwQ\n00wmk77XMBgMBnOxiC58fn4WZ9O+ceDj46P62jnU/Hi8NO9vm9FoVH3t/f19hyuZiWb6U+xggCCB\nAWIEBogRGCBGYIAYgQFiFu7sc5Gtr6/3vQSYKTsYIEZggBiBAWIEBogRGCBGYIAYgQFi3K5hhi4u\nLlrn4/G4ODs6OirOtre3q9cUtHK3a2ia/7qDwT+13eZjmp6+ccLtGoB+CQwQIzBAjMAAMQIDxAgM\nEOOYeoaenp5a51dXV8XZzc1NcdbTMeU0jql/wTE1wC8JDBAjMECMwAAxAgPECAwQ45ialJU7pl4x\njqmBfgkMECMwQIzAADECA8QIDBAjMECMwAAxAgPECAwQIzBAjMAAMQIDxAgMEDPsewHf6m/HziLw\n/q4oOxggRmCAGIEBYgQGiBEYIEZggBiBAWIEBogRGCBGYIAYgQFiBAaIERggRmCAGIEBYgQGiBEY\nIEZggBiBAWIEBogRGCBGYIAYgQFi/gByo1kzfWWm/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1673a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "nb_rows = 1\n",
    "nb_cols = 2\n",
    "nb_elements = nb_rows * nb_cols\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "grid = AxesGrid(fig, 111, # similar to subplot(141)\n",
    "                nrows_ncols = (nb_rows, nb_cols),\n",
    "                axes_pad = 0.05,\n",
    "                label_mode = \"1\",\n",
    ")\n",
    "grid[0].imshow(\n",
    "    resize(resize(image, (16, 16)), (224, 224), order=0),\n",
    "    cmap='Greys', interpolation='nearest')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(\n",
    "    resize(resize(image, (8, 8)), (224, 224), order=0),\n",
    "    cmap='Greys', interpolation='nearest')\n",
    "grid[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we rotate the image. Explore options in skimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAADICAYAAAD1NBdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c16b10590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.transform import rotate\n",
    "\n",
    "nb_rows = 1\n",
    "nb_cols = 4\n",
    "nb_elements = nb_rows * nb_cols\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "grid = AxesGrid(fig, 111, # similar to subplot(141)\n",
    "                nrows_ncols = (nb_rows, nb_cols),\n",
    "                axes_pad = 0.05,\n",
    "                label_mode = \"1\",\n",
    ")\n",
    "grid[0].imshow(rotate(image, 30), cmap='Greys', interpolation='nearest')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(rotate(image, 45), cmap='Greys', interpolation='nearest')\n",
    "grid[1].axis('off')\n",
    "grid[2].imshow(rotate(image, 60), cmap='Greys', interpolation='nearest')\n",
    "grid[2].axis('off')\n",
    "grid[3].imshow(rotate(image, 75), cmap='Greys', interpolation='nearest')\n",
    "grid[3].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these tansformations should be implemented in the transform function found in the `image_preprocessor` workflow element that you will submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline\n",
    "\n",
    "For submitting at the [RAMP site](http://ramp.studio), you will have to write a single ImageClassifier class implementing a `fit` and a `predict_proba` function.\n",
    "\n",
    "Note that the following code cells are *not* executed in the notebook. The notebook saves their contents in the file specified in the first line of the cell, so you can edit your submission before running the local test below and submitting it at the RAMP site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The starting kit image classifier\n",
    "\n",
    "The starting kit implements a simple keras neural net. Since MNIST is a small set of small images, we can actually load them into the memory. MNIST contains well-centered and aligned images so `_transform` only needs to scale the pixels into `[0, 1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/starting_kit/image_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/starting_kit/image_classifier.py\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "class ImageClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        inp = Input((28, 28, 1))\n",
    "        x = Flatten(name='flatten')(inp)\n",
    "        x = Dense(100, activation='relu', name='fc1')(x)\n",
    "        out = Dense(10, activation='softmax', name='predictions')(x)\n",
    "        self.model = Model(inp, out)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=SGD(lr=1e-4),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def _transform(self, x):\n",
    "        # adding channel dimension at the last position\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        # bringing input between 0 and 1\n",
    "        x = x / 255.\n",
    "        return x\n",
    "\n",
    "    def fit(self, img_loader):\n",
    "        # load the full data into memory\n",
    "        nb = len(img_loader)\n",
    "        # make a 4D tensor:\n",
    "        # number of images x width x height x number of channels\n",
    "        X = np.zeros((nb, 28, 28, 1))\n",
    "        # one-hot encoding of the labels to set NN target\n",
    "        Y = np.zeros((nb, 10))\n",
    "        for i in range(nb):\n",
    "            x, y = img_loader.load(i)\n",
    "            X[i] = self._transform(x)\n",
    "            # since labels are [0, ..., 9], label is the same as label index\n",
    "            Y[i, y] = 1\n",
    "        self.model.fit(X, Y, batch_size=32, validation_split=0.1, epochs=1)\n",
    "\n",
    "    def predict_proba(self, img_loader):\n",
    "        nb = len(img_loader)\n",
    "        X = np.zeros((nb, 28, 28, 1))\n",
    "        for i in range(nb):\n",
    "            X[i] = self._transform(img_loader.load(i))\n",
    "        return self.model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple keras convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/keras_convnet/image_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/keras_convnet/image_classifier.py\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class ImageClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        inp = Input((28, 28, 1))\n",
    "        # Block 1\n",
    "        x = Conv2D(\n",
    "            32, (3, 3), activation='relu', padding='same',\n",
    "            name='block1_conv1')(inp)\n",
    "        x = Conv2D(\n",
    "            32, (3, 3), activation='relu', padding='same',\n",
    "            name='block1_conv2')(x)\n",
    "        x = MaxPooling2D(\n",
    "            (2, 2), strides=(2, 2),\n",
    "            name='block1_pool')(x)\n",
    "        # dense\n",
    "        x = Flatten(\n",
    "            name='flatten')(x)\n",
    "        x = Dense(\n",
    "            512, activation='relu',\n",
    "            name='fc1')(x)\n",
    "        out = Dense(10, activation='softmax', name='predictions')(x)\n",
    "        self.model = Model(inp, out)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def _transform(self, x):\n",
    "        # adding channel dimension at the last position\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        # bringing input between 0 and 1\n",
    "        x = x / 255.\n",
    "        return x\n",
    "\n",
    "    def fit(self, img_loader):\n",
    "        # load the full data into memory\n",
    "        nb = len(img_loader)\n",
    "        # make a 4D tensor:\n",
    "        # number of images x width x height x number of channels\n",
    "        X = np.zeros((nb, 28, 28, 1))\n",
    "        # one-hot encoding of the labels to set NN target\n",
    "        Y = np.zeros((nb, 10))\n",
    "        for i in range(nb):\n",
    "            x, y = img_loader.load(i)\n",
    "            X[i] = self._transform(x)\n",
    "            # since labels are [0, ..., 9], label is the same as label index\n",
    "            Y[i, y] = 1\n",
    "        self.model.fit(X, Y, batch_size=32, validation_split=0.1, epochs=1)\n",
    "\n",
    "    def predict_proba(self, img_loader):\n",
    "        nb = len(img_loader)\n",
    "        X = np.zeros((nb, 28, 28, 1))\n",
    "        for i in range(nb):\n",
    "            X[i] = self._transform(img_loader.load(i))\n",
    "        return self.model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple pytorch convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/pytorch_convnet/image_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/pytorch_convnet/image_classifier.py\n",
    "from __future__ import division\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def _make_variable(X):\n",
    "    variable = Variable(torch.from_numpy(X))\n",
    "    if is_cuda:\n",
    "        variable = variable.cuda()\n",
    "    return variable\n",
    "\n",
    "\n",
    "def _flatten(x):\n",
    "    return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 14 * 14, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = _flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Source: https://github.com/pytorch/vision/blob/master/torchvision/\n",
    "        # models/vgg.py\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class ImageClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.net = Net()\n",
    "        if is_cuda:\n",
    "            self.net = self.net.cuda()\n",
    "\n",
    "    def _transform(self, x):\n",
    "        # adding channel dimension at the first position\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # bringing input between 0 and 1\n",
    "        x = x / 255.\n",
    "        return x\n",
    "\n",
    "    def _get_acc(self, y_pred, y_true):\n",
    "        y_pred = y_pred.cpu().data.numpy().argmax(axis=1)\n",
    "        y_true = y_true.cpu().data.numpy()\n",
    "        return (y_pred == y_true)\n",
    "\n",
    "    def _load_minibatch(self, img_loader, indexes):\n",
    "        n_minibatch_images = len(indexes)\n",
    "        X = np.zeros((n_minibatch_images, 1, 28, 28), dtype=np.float32)\n",
    "        # one-hot encoding of the labels to set NN target\n",
    "        y = np.zeros(n_minibatch_images, dtype=np.int)\n",
    "        for i, i_load in enumerate(indexes):\n",
    "            x, y[i] = img_loader.load(i_load)\n",
    "            X[i] = self._transform(x)\n",
    "            # since labels are [0, ..., 9], label is the same as label index\n",
    "        X = _make_variable(X)\n",
    "        y = _make_variable(y)\n",
    "        return X, y\n",
    "\n",
    "    def _load_test_minibatch(self, img_loader, indexes):\n",
    "        n_minibatch_images = len(indexes)\n",
    "        X = np.zeros((n_minibatch_images, 1, 28, 28), dtype=np.float32)\n",
    "        for i, i_load in enumerate(indexes):\n",
    "            x = img_loader.load(i_load)\n",
    "            X[i] = self._transform(x)\n",
    "        X = _make_variable(X)\n",
    "        return X\n",
    "\n",
    "    def fit(self, img_loader):\n",
    "        validation_split = 0.1\n",
    "        batch_size = 32\n",
    "        nb_epochs = 1\n",
    "        lr = 1e-4\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        if is_cuda:\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            t0 = time.time()\n",
    "            self.net.train()  # train mode\n",
    "            nb_trained = 0\n",
    "            nb_updates = 0\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            n_images = len(img_loader) * (1 - validation_split)\n",
    "            i = 0\n",
    "            while i < n_images:\n",
    "                indexes = range(i, min(i + batch_size, n_images))\n",
    "                X, y = self._load_minibatch(img_loader, indexes)\n",
    "                i += len(indexes)\n",
    "                # zero-out the gradients because they accumulate by default\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.net(X)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step()  # update params\n",
    "\n",
    "                # Loss and accuracy\n",
    "                train_acc.extend(self._get_acc(y_pred, y))\n",
    "                train_loss.append(loss.data[0])\n",
    "                nb_trained += X.size(0)\n",
    "                nb_updates += 1\n",
    "                if nb_updates % 100 == 0:\n",
    "                    print(\n",
    "                        'Epoch [{}/{}], [trained {}/{}], avg_loss: {:.4f}'\n",
    "                        ', avg_train_acc: {:.4f}'.format(\n",
    "                            epoch + 1, nb_epochs, nb_trained, n_images,\n",
    "                            np.mean(train_loss), np.mean(train_acc)))\n",
    "\n",
    "            self.net.eval()  # eval mode\n",
    "            valid_acc = []\n",
    "            n_images = len(img_loader)\n",
    "            while i < n_images:\n",
    "                indexes = range(i, min(i + batch_size, n_images))\n",
    "                X, y = self._load_minibatch(img_loader, indexes)\n",
    "                i += len(indexes)\n",
    "                y_pred = self.net(X)\n",
    "                valid_acc.extend(self._get_acc(y_pred, y))\n",
    "\n",
    "            delta_t = time.time() - t0\n",
    "            print('Finished epoch {}'.format(epoch + 1))\n",
    "            print('Time spent : {:.4f}'.format(delta_t))\n",
    "            print('Train acc : {:.4f}'.format(np.mean(train_acc)))\n",
    "            print('Valid acc : {:.4f}'.format(np.mean(valid_acc)))\n",
    "\n",
    "    def predict_proba(self, img_loader):\n",
    "        batch_size = 32\n",
    "        n_images = len(img_loader)\n",
    "        i = 0\n",
    "        y_proba = np.empty((n_images, 10))\n",
    "        while i < n_images:\n",
    "            indexes = range(i, min(i + batch_size, n_images))\n",
    "            X = self._load_test_minibatch(img_loader, indexes)\n",
    "            i += len(indexes)\n",
    "            y_proba[indexes] = nn.Softmax()(self.net(X)).cpu().data.numpy()\n",
    "        return y_proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing (before submission)\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. For this we provide a unit test. Note that the test runs on your files in [`submissions/starting_kit`](/tree/submissions/starting_kit), not on the classes defined in the cells of this notebook.\n",
    "\n",
    "First `pip install ramp-workflow` or install it from the [github repo](https://github.com/paris-saclay-cds/ramp-workflow). Make sure that the python file `image_classifier.py` is in the  [`submissions/starting_kit`](/tree/submissions/starting_kit) folder, and the data `train.csv` and `test.csv` are in [`data`](/tree/data). If you haven't yet, download the images by executing `python download_data.py`. Then run\n",
    "\n",
    "```ramp_test_submission```\n",
    "\n",
    "If it runs and print training and test errors on each fold, then you can submit the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MNIST classification\n",
      "Reading train and test files from ./data ...\n",
      "Reading cv ...\n",
      "Training ./submissions/starting_kit ...\n",
      "Using TensorFlow backend.\n",
      "Train on 34560 samples, validate on 3840 samples\n",
      "Epoch 1/1\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "34560/34560 [==============================] - 3s - loss: 2.3441 - acc: 0.1071 - val_loss: 2.2978 - val_acc: 0.1289\n",
      "CV fold 0\n",
      "\ttrain acc = 0.132\n",
      "\tvalid acc = 0.132\n",
      "\ttest acc = 0.129\n",
      "\ttrain nll = 2.294\n",
      "\tvalid nll = 2.294\n",
      "\ttest nll = 2.293\n",
      "----------------------------\n",
      "train acc = 0.132 ± 0.0\n",
      "train nll = 2.294 ± 0.0\n",
      "valid acc = 0.132 ± 0.0\n",
      "valid nll = 2.294 ± 0.0\n",
      "test acc = 0.129 ± 0.0\n",
      "test nll = 2.293 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "!ramp_test_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good feature extractor and classifier, you can submit them to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then find an open event on the particular problem, for example, the event [MNIST](http://www.ramp.studio/events/MNIST_simplified) for this RAMP. Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/MNIST_simplified/sandbox) and copy-paste (or upload) [`image_preprocessor.py`](/edit/submissions/starting_kit/image_preprocessor.py) and [`batch_classifier.py`](/edit/submissions/starting_kit/batch_classifier.py) from `submissions/starting_kit`. Save it, rename it, then submit it. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/MNIST_simplified/my_submissions). Once it is trained, you get a mail, and your submission shows up on the [public leaderboard](http://www.ramp.studio/events/MNIST_simplified/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/MNIST_simplified/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, perhaps do some AutoML/hyperopt, etc., _locally_, and checking them with `ramp_test_submission`. The script prints mean cross-validation scores \n",
    "```\n",
    "----------------------------\n",
    "train acc = 0.132 ± 0.0\n",
    "train nll = 2.294 ± 0.0\n",
    "valid acc = 0.132 ± 0.0\n",
    "valid nll = 2.294 ± 0.0\n",
    "test acc = 0.129 ± 0.0\n",
    "test nll = 2.293 ± 0.0\n",
    "```\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/MNIST_simplified/leaderboard)) is balanced accuracy aka macro-averaged recall, so the line that is relevant in the output of `ramp_test_submission` is `valid acc = 0.132 ± 0.0`. When the score is good enough, you can submit it at the RAMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=MNIST simplified notebook)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
